{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Include any disclaimers about the use of AI and cite the parts of the code that AI have been used for.**"
      ],
      "metadata": {
        "id": "vsXP1OTGoZH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if not sys.version.startswith(\"3.12\"):\n",
        "    raise RuntimeError(\"This notebook requires Python 3.12\")\n",
        "\n",
        "!pip install numpy pandas matplotlib seaborn scikit-learn torch platform psutil getpass\n",
        "\n",
        "# Hardware and user spercifications\n",
        "\n",
        "import platform\n",
        "import psutil\n",
        "import getpass\n",
        "\n",
        "# --- Setup: Imports ---\n",
        "import os, seaborn, sklearn, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#CODE HERE (optional): import any module needed from sklearn package\n",
        "\n",
        "def print_system_info():\n",
        "    print(\"=\"*60)\n",
        "    print(\"SYSTEM INFORMATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # OS and hardware\n",
        "    print(f\"Operating System : {platform.system()} {platform.release()}\")\n",
        "    print(f\"Processor        : {platform.processor()}\")\n",
        "    print(f\"CPU Cores        : {psutil.cpu_count(logical=True)}\")\n",
        "    print(f\"RAM              : {round(psutil.virtual_memory().total / (1024**3), 2)} GB\")\n",
        "\n",
        "    # User\n",
        "    print(f\"PC Username      : {getpass.getuser()}\")\n",
        "\n",
        "    # Python and environment\n",
        "    print(f\"Python Version   : {platform.python_version()}\")\n",
        "    print(f\"Working Dir      : {os.getcwd()}\")\n",
        "\n",
        "    # GPU (if PyTorch is installed)\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU Device       : {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"CUDA Version     : {torch.version.cuda}\")\n",
        "    else:\n",
        "        print(\"GPU Device       : None (CPU mode)\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print()\n",
        "\n",
        "print_system_info()\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)"
      ],
      "metadata": {
        "id": "HgPCY8Stpa3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we load, split, scale the data. Then we convert them to PyTorch Tensors."
      ],
      "metadata": {
        "id": "gNdvXOuopyXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Loading and Preprocessing ---\n",
        "\n",
        "# CODE HERE: Load the dataset\n",
        "\n",
        "\n",
        "# CODE HERE: Split the data (80% train, 20% validation). Set random_state=random_seed.\n",
        "\n",
        "\n",
        "# CODE HERE: Scale the features\n",
        "\n",
        "\n",
        "# CODE HERE: Convert to PyTorch Tensors\n",
        "\n",
        "\n",
        "print(f\"Training features shape: {X_train_t.shape}\")\n",
        "print(f\"Validation features shape: {X_val_t.shape}\")"
      ],
      "metadata": {
        "id": "8JKvTXTgpyhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the Neural Network (NN)."
      ],
      "metadata": {
        "id": "O7NbSrUWspc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Definition ---\n",
        "\n",
        "# CODE HERE: define the asked NN here\n",
        "\n",
        "input_features = X_train_t.shape[1]\n",
        "print(f\"Model will accept {input_features} input features.\")\n",
        "print(\"--- 3. Model Class Defined ---\")"
      ],
      "metadata": {
        "id": "XLc_-U1yskWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1:"
      ],
      "metadata": {
        "id": "I4_psABYtvF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_full_loss(model, criterion, X, y):\n",
        "    \"\"\"Helper function to calculate loss over an entire dataset.\"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        outputs = model(X)\n",
        "        loss = criterion(outputs, y)\n",
        "    model.train() # Set model back to train mode\n",
        "    return loss.item()\n",
        "\n",
        "def stochastic_gradient_descent(model, criterion, optimizer, X_train, y_train, X_val, y_val,\n",
        "                                max_iterations, check_every, patience, threshold):\n",
        "\n",
        "    #CODE HERE: fill function. Use calculate_full_loss function for loss.\n",
        "\n",
        "    return train_losses, val_losses, iterations, stop_iteration, best_model_state"
      ],
      "metadata": {
        "id": "YmnSTH7EtvQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and plot:"
      ],
      "metadata": {
        "id": "GG9b0eujudgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hyperparameters for Part 1 ---\n",
        "LEARNING_RATE =\n",
        "MAX_ITERATIONS =\n",
        "CHECK_EVERY =\n",
        "PATIENCE =\n",
        "THRESHOLD = 1e-4\n",
        "\n",
        "# --- Model Initialization ---\n",
        "\n",
        "#CODE HERE\n",
        "\n",
        "# --- Run Training ---\n",
        "\n",
        "#CODE HERE\n",
        "\n",
        "# --- Plotting Results ---\n",
        "print(\"Plotting Part 1 results...\")\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(iterations_sgd, train_losses_sgd, label='Training Loss', marker='o')\n",
        "plt.plot(iterations_sgd, val_losses_sgd, label='Validation Loss', marker='x')\n",
        "plt.axvline(x=stop_iter, color='red', linestyle='--',\n",
        "            label=f'Early Stopping at {stop_iter} iterations')\n",
        "plt.title('Part 1: SGD Training and Validation Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss (BCELoss)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Final Accuracy ---\n",
        "\n",
        "#CODE HERE\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    #CODE HERE\n",
        "\n",
        "    print(f\"\\nAccuracy of Early-Stopped Model on Validation Set: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "uEt7ooDnudth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2:"
      ],
      "metadata": {
        "id": "9ytQhDiQvVOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_minibatch(model, criterion, optimizer, X_train, y_train, X_val, y_val,\n",
        "                         num_iterations, batch_size, check_every):\n",
        "\n",
        "    #CODE HERE: fill function\n",
        "\n",
        "    return train_losses, val_losses, iterations, model # Return the final model\n"
      ],
      "metadata": {
        "id": "qT1i_oXvvVVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and plot:"
      ],
      "metadata": {
        "id": "-WtycXbwvvkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hyperparameters for Part 2 ---\n",
        "LEARNING_RATE_MB =\n",
        "NUM_ITERATIONS_MB =\n",
        "BATCH_SIZE =\n",
        "CHECK_EVERY_MB =\n",
        "\n",
        "# --- Model Initialization ---\n",
        "\n",
        "#CODE HERE\n",
        "\n",
        "# --- Run Minibatch Training ---\n",
        "\n",
        "#CODE HERE\n",
        "\n",
        "# --- Plotting Comparison ---\n",
        "print(\"Plotting Part 2 comparison...\")\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# SGD (Part 1)\n",
        "plt.plot(iterations_sgd, train_losses_sgd, label='SGD - Train Loss', linestyle=':', color='blue', marker='o')\n",
        "plt.plot(iterations_sgd, val_losses_sgd, label='SGD - Validation Loss', linestyle='-', color='blue', marker='x')\n",
        "\n",
        "# Minibatch (Part 2)\n",
        "plt.plot(iterations_mb, train_losses_mb, label='Minibatch - Train Loss', linestyle=':', color='green', marker='o')\n",
        "plt.plot(iterations_mb, val_losses_mb, label='Minibatch - Validation Loss', linestyle='-', color='green', marker='x')\n",
        "\n",
        "plt.title('Part 2: SGD vs. Minibatch SGD Loss Comparison')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss (BCELoss)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sos86lsEvvu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance evaluation:"
      ],
      "metadata": {
        "id": "QzUMCeetwSAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Performance Metrics Comparison ---\n",
        "\n",
        "def get_metrics(model, X_val, y_val):\n",
        "    \"\"\"Helper function to get accuracy, precision, and recall.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_prob = model(X_val)\n",
        "        y_pred = (y_pred_prob > 0.5).int().numpy() # Convert to numpy\n",
        "        y_true = y_val.numpy()\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        pre = precision_score(y_true, y_pred)\n",
        "        rec = recall_score(y_true, y_pred)\n",
        "    return acc, pre, rec\n",
        "\n",
        "# Get metrics for Part 1 (Early-Stopped SGD Model)\n",
        "\n",
        "#CODE HERE: USING get_metrics function\n",
        "\n",
        "# Get metrics for Part 2 (Final Minibatch Model)\n",
        "\n",
        "#CODE HERE: USING get_metrics function\n",
        "\n",
        "# --- Show Results in a Table ---\n",
        "results = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall'],\n",
        "    'Part 1: Early-Stopped SGD': [f\"{acc_sgd:.4f}\", f\"{pre_sgd:.4f}\", f\"{rec_sgd:.4f}\"],\n",
        "    'Part 2: Minibatch SGD': [f\"{acc_mb:.4f}\", f\"{pre_mb:.4f}\", f\"{rec_mb:.4f}\"]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n--- Final Performance Metrics on Validation Set ---\")\n",
        "\n",
        "from IPython.display import display\n",
        "display(results_df)\n"
      ],
      "metadata": {
        "id": "E0qei9agwSHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPLAIN HERE: Based on your experiments, answer the following questions:\n",
        "\n",
        "1) How does early stopping affect the training and validation loss curves compared to training for all 5000 iterations?\n",
        "2) How does minibatch SGD affect the smoothness of loss curves compared to true SGD with batch_size=1?\n"
      ],
      "metadata": {
        "id": "zg2Ohbck3xtW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Part 3***"
      ],
      "metadata": {
        "id": "qvXNfmHK-l85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetWithDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    TODO: Complete this neural network class to include dropout layers.\n",
        "\n",
        "    The dropout_rate parameter should control the dropout probability.\n",
        "    When dropout_rate=0.0, no dropout is applied.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, dropout_rate=0.0):\n",
        "        super(FeedForwardNetWithDropout, self).__init__()\n",
        "\n",
        "        #CODE HERE: Define the NN architecture\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        TODO: Implement the forward pass.\n",
        "\n",
        "        Return the final output.\n",
        "        \"\"\"\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AliA5ZlP-hMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, X, y):\n",
        "    \"\"\"\n",
        "    TODO: Implement accuracy calculation.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    # CODE HERE: Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # CODE HERE: Get model outputs\n",
        "        outputs = None  # REPLACE WITH YOUR CODE\n",
        "        predictions = None  # REPLACE WITH YOUR CODE\n",
        "        accuracy = None  # REPLACE WITH YOUR CODE\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "LXpdLP21-hS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_dropout(model, criterion, optimizer, X_train, y_train, X_val, y_val,\n",
        "                                 num_iterations, batch_size, check_every):\n",
        "    \"\"\"\n",
        "    TODO: Complete this training function to support dropout.\n",
        "\n",
        "    \"\"\"\n",
        "    # CODE HERE: Use need to fill like using miniSGD in part 2\n",
        "\n",
        "        return train_losses, val_losses, train_accs, val_accs, iterations"
      ],
      "metadata": {
        "id": "WzoFmHIk_a6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DROPOUT_RATES =\n",
        "LEARNING_RATE_DROPOUT =\n",
        "NUM_ITERATIONS_DROPOUT =\n",
        "BATCH_SIZE_DROPOUT =\n",
        "CHECK_EVERY_DROPOUT =\n",
        "\n",
        "dropout_results = {}\n",
        "\n",
        "for dropout_rate in DROPOUT_RATES:\n",
        "    print(f\"\\n--- Training with Dropout Rate = {dropout_rate} ---\")\n",
        "\n",
        "    # CODE HERE: Initialize model with current dropout_rate\n",
        "    model = None  # REPLACE WITH YOUR CODE\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    train_losses, val_losses, train_accs, val_accs, iterations = train_with_minibatch_dropout(\n",
        "        model, criterion, optimizer,\n",
        "        X_train_t, y_train_t, X_val_t, y_val_t,\n",
        "        NUM_ITERATIONS_DROPOUT, BATCH_SIZE_DROPOUT, CHECK_EVERY_DROPOUT\n",
        "    )\n",
        "\n",
        "    # Store results for later comparison\n",
        "    dropout_results[dropout_rate] = {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'val_accs': val_accs,\n",
        "        'iterations': iterations,\n",
        "        'model': model\n",
        "    }\n",
        "\n",
        "    print(f\"Final Training Accuracy: {train_accs[-1]*100:.2f}%\")\n",
        "    print(f\"Final Validation Accuracy: {val_accs[-1]*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "VmmnlYTDAd87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot accuracies/validations vs dropouts\n",
        "# CODE HERE: inject points in plots\n",
        "\n",
        "axes[0].set_title('Part 3: Training Accuracy vs Dropout Rate', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Iterations')\n",
        "axes[0].set_ylabel('Training Accuracy (%)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "# CODE HERE: inject points in plots\n",
        "\n",
        "axes[1].set_title('Part 3: Validation Accuracy vs Dropout Rate', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Iterations')\n",
        "axes[1].set_ylabel('Validation Accuracy (%)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "\n",
        "axes[0].set_title('Part 3: Training Loss vs Dropout Rate', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Iterations')\n",
        "axes[0].set_ylabel('Training Loss (BCE)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].set_title('Part 3: Validation Loss vs Dropout Rate', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Iterations')\n",
        "axes[1].set_ylabel('Validation Loss (BCE)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G2Y3tvM3AenI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE HERE: Create a summary dataframe with the following columns:\n",
        "\n",
        "dropout_summary = {\n",
        "    'Dropout Rate': [],\n",
        "    'Final Train Acc (%)': [],\n",
        "    'Final Val Acc (%)': [],\n",
        "    'Final Train Loss': [],\n",
        "    'Final Val Loss': []\n",
        "}\n",
        "\n",
        "# CODE HERE: Fill the summary dictionary by looping through DROPOUT_RATES\n",
        "\n",
        "dropout_df = pd.DataFrame(dropout_summary)\n",
        "display(dropout_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "lUUcssX_AxBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXPLAIN HERE: Based on your results, answer the following questions:\n",
        "\n",
        "\n",
        "1. Which dropout rate shows the best generalization (smallest gap between\n",
        "   training and validation accuracy)?\n",
        "\n",
        "\n",
        "2. Which dropout rate would you recommend for this dataset and why?\n",
        "\n",
        "\n",
        "3. What are the trade-offs between low dropout (0.1) and high dropout (0.5)?\n"
      ],
      "metadata": {
        "id": "nlcHugmkAuZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Part 4***"
      ],
      "metadata": {
        "id": "QIfvk6juBAkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_minibatch_l2(model, criterion, optimizer, X_train, y_train, X_val, y_val,\n",
        "                            num_iterations, batch_size, check_every):\n",
        "    \"\"\"\n",
        "    #TODO: Implement training with L2 regularization.\n",
        "\n",
        "    \"\"\"\n",
        "    n_samples = X_train.shape[0]\n",
        "    train_losses, val_losses, iterations = [], [], []\n",
        "\n",
        "    X_train_shuffled = X_train.clone()\n",
        "    y_train_shuffled = y_train.clone()\n",
        "    current_idx = 0\n",
        "\n",
        "    for i in range(num_iterations + 1):\n",
        "        if current_idx + batch_size > n_samples:\n",
        "            X_train_np, y_train_np = shuffle(X_train_shuffled.numpy(), y_train_shuffled.numpy(),\n",
        "                                            random_state=random_seed + i)\n",
        "            X_train_shuffled = torch.tensor(X_train_np, dtype=torch.float32)\n",
        "            y_train_shuffled = torch.tensor(y_train_np, dtype=torch.float32)\n",
        "            current_idx = 0\n",
        "\n",
        "        X_batch = X_train_shuffled[current_idx : current_idx + batch_size]\n",
        "        y_batch = y_train_shuffled[current_idx : current_idx + batch_size]\n",
        "        current_idx += batch_size\n",
        "\n",
        "        # CODE HERE: Complete the training\n",
        "\n",
        "        if i % check_every == 0:\n",
        "            # CODE HERE: Calculate and store training and validation losses\n",
        "            pass\n",
        "\n",
        "    return train_losses, val_losses, iterations\n"
      ],
      "metadata": {
        "id": "IWZU2DQMBjRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_minibatch_l1(model, criterion, optimizer, X_train, y_train, X_val, y_val,\n",
        "                            num_iterations, batch_size, check_every, lambda_l1):\n",
        "    \"\"\"\n",
        "    TODO: Implement training with manual L1 regularization.\n",
        "    \"\"\"\n",
        "    n_samples = X_train.shape[0]\n",
        "    train_losses, val_losses, iterations = [], [], []\n",
        "\n",
        "    X_train_shuffled = X_train.clone()\n",
        "    y_train_shuffled = y_train.clone()\n",
        "    current_idx = 0\n",
        "\n",
        "    for i in range(num_iterations + 1):\n",
        "        if current_idx + batch_size > n_samples:\n",
        "            X_train_np, y_train_np = shuffle(X_train_shuffled.numpy(), y_train_shuffled.numpy(),\n",
        "                                            random_state=random_seed + i)\n",
        "            X_train_shuffled = torch.tensor(X_train_np, dtype=torch.float32)\n",
        "            y_train_shuffled = torch.tensor(y_train_np, dtype=torch.float32)\n",
        "            current_idx = 0\n",
        "\n",
        "        X_batch = X_train_shuffled[current_idx : current_idx + batch_size]\n",
        "        y_batch = y_train_shuffled[current_idx : current_idx + batch_size]\n",
        "        current_idx += batch_size\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # CODE HERE: Calculate outputs and BCE loss\n",
        "        outputs = None  # REPLACE WITH YOUR CODE\n",
        "        loss = None  # REPLACE WITH YOUR CODE\n",
        "\n",
        "        # CODE HERE: Calculate L1 penalty\n",
        "\n",
        "        l1_penalty = 0\n",
        "\n",
        "        # CODE HERE: Calculate total loss\n",
        "        total_loss = None  # REPLACE WITH YOUR CODE\n",
        "\n",
        "        # TODO: Backpropagate and update weights using total_loss\n",
        "\n",
        "        if i % check_every == 0:\n",
        "            # CODE HERE: Calculate losses\n",
        "            pass\n",
        "\n",
        "    return train_losses, val_losses, iterations"
      ],
      "metadata": {
        "id": "aXDr0XbfCD7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LAMBDA_L2_VALUES =\n",
        "LEARNING_RATE_L2 =\n",
        "NUM_ITERATIONS_L2 =\n",
        "BATCH_SIZE_L2 =\n",
        "CHECK_EVERY_L2 =\n",
        "\n",
        "l2_results = {}\n",
        "\n",
        "for lambda_l2 in LAMBDA_L2_VALUES:\n",
        "    print(f\"\\n--- Training with L2 λ = {lambda_l2} ---\")\n",
        "\n",
        "    # CODE HERE: Initialize & train the model (use original FeedForwardNet, no dropout)\n",
        "    model =\n",
        "\n",
        "    train_losses, val_losses, iterations = train_with_minibatch_l2(\n",
        "        model, criterion, optimizer,\n",
        "        X_train_t, y_train_t, X_val_t, y_val_t,\n",
        "        NUM_ITERATIONS_L2, BATCH_SIZE_L2, CHECK_EVERY_L2\n",
        "    )\n",
        "\n",
        "    # CODE HERE: Calculate final validation accuracy\n",
        "    final_accuracy =\n",
        "\n",
        "    # Store results\n",
        "    l2_results[lambda_l2] = {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'iterations': iterations,\n",
        "        'final_accuracy': final_accuracy,\n",
        "        'model': model\n",
        "    }\n",
        "\n",
        "    print(f\"Final Validation Accuracy: {final_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "Z90nnVEjCL49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# TODO: Plot training/validation loss for all L2 values\n",
        "# CODE HERE: Inject points to plots\n",
        "\n",
        "axes[0].set_title('Part 4a: Training Loss with L2 Regularization', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Iterations')\n",
        "axes[0].set_ylabel('Training Loss (BCE)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# CODE HERE: Inject points to plots\n",
        "\n",
        "axes[1].set_title('Part 4a: Validation Loss with L2 Regularization', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Iterations')\n",
        "axes[1].set_ylabel('Validation Loss (BCE)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lq2zHXIECSeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LAMBDA_L1 = 1e-4\n",
        "\n",
        "# CODE HERE : Initialize model, criterion, and optimizer for L1 experiment\n",
        "model_l1 =\n",
        "criterion_l1 =\n",
        "optimizer_l1 =\n",
        "\n",
        "# Train with L1\n",
        "train_losses_l1, val_losses_l1, iterations_l1 = train_with_minibatch_l1(\n",
        "    model_l1, criterion_l1, optimizer_l1,\n",
        "    X_train_t, y_train_t, X_val_t, y_val_t,\n",
        "    NUM_ITERATIONS_L2, BATCH_SIZE_L2, CHECK_EVERY_L2, LAMBDA_L1\n",
        ")\n",
        "\n",
        "# CODE HERE: Calculate final validation accuracy for L1 model\n",
        "final_accuracy_l1 =\n",
        "\n",
        "print(f\"Final Validation Accuracy (L1): {final_accuracy_l1*100:.2f}%\")"
      ],
      "metadata": {
        "id": "jlzvwaOsCVcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# CODE HERE: inject points to plots\n",
        "\n",
        "axes[0].set_title('Part 4b: Training Loss - L1 vs L2', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Iterations')\n",
        "axes[0].set_ylabel('Training Loss (BCE)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# CODE HERE: inject points to plots\n",
        "\n",
        "axes[1].set_title('Part 4b: Validation Loss - L1 vs L2', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Iterations')\n",
        "axes[1].set_ylabel('Validation Loss (BCE)')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LnlvprI6CaWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TODO: Create a summary dataframe with columns:\n",
        "# Fill rows for: No regularization, L2 (1e-4), L2 (1e-3), L1 (1e-4)\n",
        "# CODE HERE\n",
        "\n",
        "reg_summary = {\n",
        "    'Regularization': [],\n",
        "    'Final Val Acc (%)': [],\n",
        "    'Final Train Loss': [],\n",
        "    'Final Val Loss': [],\n",
        "    'Sparsity (%)': []\n",
        "}\n",
        "\n",
        "# TODO: Fill the summary dictionary\n",
        "# CODE HERE\n",
        "\n",
        "reg_df = pd.DataFrame(reg_summary)\n",
        "display(reg_df)\n"
      ],
      "metadata": {
        "id": "3FTs7iHPC2y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#EXPLAIN HERE: Based on your experiments, answer the following questions:\n",
        "\n",
        "\n",
        "1. What happens when you increase L2 strength from λ=1e-4 to λ=1e-3?\n",
        "\n",
        "   \n",
        "2. Which regularization method (L1/L2 or no regulation) achieves better generalization for this breast cancer dataset?\n",
        "   "
      ],
      "metadata": {
        "id": "Y5eI3uDrC9a2"
      }
    }
  ]
}